The horribly complex make process, part III :-)
==============================================

As mentioned before the make process has to have the following properties:
- it works
- it is fast, ie it does not do things which it does not need to do in
  order to complete the build
- it needs as few extra tools to build as possible.

My suggestion, as outlined below, is based on using a single makefile 
concept. This allows it to fit the second and third requirements,
whether it fits the first (and probably most important) requirement is
yet to be seen.

How does it work?
-----------------
In order for this to work, every code module that can possibly build
something should define a file called Makefile.inc[1]. This file will
be conditionally read into the make process by some rules defined
in the file $(TOP)/config/$(ARCH)-$(CPU)/Makefile.inc. This makefile is
specific to the target operating system.

As an example, this is what a FreeBSD system might have (for the
moment not considering any device drivers):

KERNEL_MODS :=
	aros battclock boot dos exec expansion filesys hidd graphics \
	intuition keymap layers mathffp mathieeesingbas oop timer \
	utility 

kernel : arosshell

This variable will be taken by the main Makefile with a construction
something like this:

arosshell : arosshell.c $(foreach f, $(KERNEL_MODS), $(LIBDIR)/lib$(f).a)

So by calling make kernel, you will automatically build all the required
kernel modules. Not that the kernel target here is a control target, rather
than one which actually builds a file. Because the kernel can take
different forms under different kinds of system (it might be a monolithic
kernel under an emulated system (ie arosshell), or some kind of dynamically
loaded thing using a bootloader).

Basically its a lot like the old MetaMake system, but without the extra
program.

What about machine dependance?
------------------------------
This is where it gets tricky. Now the problem before was that the makefiles
for all the different directories had no way of determining what files to
add to where. Because now everything can be seen this is considerably easier.
Take for example the exec.library; this needs a number of files which
are dependant upon both the host CPU and the host OS, assuming that a file
in the $(ARCH) directory is more important than an equivalent
file in the $(CPU) directory, we can do the following:

FILE_PATHS := os/$(ARCH) cpu/$(CPU) kernel stub
vpath
vpath %.c $(foreach f, $(FILE_PATHS), $(TOP)/src/$(f)/exec)
-include $(TOP)/src/os/$(ARCH)/Makefile.inc $(TOP)/src/cpu/$(CPU)/Makefile.inc

This will tell it to look in the $(ARCH), $(CPU), machine independant and
finally the stubs[2] directory. This allows us to specify all the of the
functions in the src/kernel/exec directory, and if a file exists in one
of the machine independant directories, the use it instead[3]. There are
also makefiles in these directories in case we need to add any extra files
into the build, which is simply does by putting them on the right hand side
of a special target. This will probably be slightly different because we
wish to give priority to a %.s before a %.c if they both exist in the same
directory.

Note that we clear the vpath before each new module because we want to make
sure that we don't get any name clashes from different modules.

Different kinds of builds
-------------------------
How does it handle different kinds of builds? Basically in the same way
that we do at the moment. If we are building to a link library then the
kernel-exec-linklib target is referenced, otherwise we would build the
kernel-exec-module target.

Problems with this system
-------------------------
The problems with this system, whilst not catastrophic, are at least
rather annoying. The biggest problem comes from no longer being able
to redefine variables like $(CFLAGS), $(INCLUDES), etc. The reason for
this is that the values of these are not substituted until they are
used, ie when make actually runs the command to satisfy a rule. So if
we declare CFLAGS in kernel/exec, but again in workbench/c we will actually
get the workbench/c version in kernel/exec, because the rules will not be
run until after the workbench/c makefile has been processed.

This is rather annoying, but can be fiddled with judicious use of genmf
and really horrible variable names (make doesn't care about the names,
so we could have a variable like

WORKBENCH_C_CFLAGS := ...

with later on
$(OBJDIR)/%.o : $(CURDIR)/%.c
		%compile_q opt=$(WORKBENCH_C_CFLAGS)

If you don't actually need to change the options to a build rule, then 
you don't have to define a command, since there can be one defined that
will compile what you want, this is of most use in the kernel though,
where the builds are all pretty much the same (or at least they should
be).

The vpath mechanism for do machine dependance is also a bit tricky,
because it makes the use of archtool much more annoying, since in order
to get the correct versions of files, we would need to unpack the archive
before we generate the functions.c file. Mind you I don't think kernel
functions should be in archives anyway, it makes the editing unwieldy,
but I do agree with compiling them that way. Archives of course are of
great use for OOP classes though, although again you would have to
unpack and recombine in order to get the correct versions if you have
to do vpath stuff (which for most OOP classes is silly, because they
will already be machine dependant ie HIDDs).

Biggest Problem
---------------
The biggest problem with this is working out where to start, since it
is a large task. Do I just copy all the makefiles from the MetaMake
variant, and try and fix up the problems mentioned above? To be 
honest it is probably a multiple person job, and will probably mean that
AROS will not build for a week or two.

Suggested Implementation Strategy
---------------------------------
1. Commit any outstanding code. (All my code is outstanding :-)

2. Freeze the code.

3. Make sure that the system builds under _ALL_ supported platforms as it
is, and if possible is even fairly bug-free. This will make things much
easier when trying to sort out obscure build problems.

4. Tag the tree and possibly even release a binary version. Archive
the CVS tree (just in case everything is stuffed up). Perhaps doing
this on a daily basis might be useful, just to be extra sure. I will
admit that CVS should handle this itself satisfactorily, but you can
never be too sure.

5. Rearrange the directory structure. This includes doing such things
as moving includes files from under compiler/includes to their proper
directories under src/kernel/blah/includes if that is desired.

6. Make sure that the code will still build using MetaMake. This will
probably involve adding some rules in order to get the includes to
work if they are moved.

7. Dearchive the code that has been combined into archives for the
reasons outlined above. Once this is done, test the build again to make
sure that it still works.

NB: When I was still using my Amiga regularly, I used GoldEd as my
editor, and being a really poor bastard[4] I couldn't load more than
1000 line files, which many large libraries would certainly manage.

8. Start converting the mmakefile.src files into Makefile.src files
working in a target by target method. It should satisfactorily build
each stage (ie setup should create all the right directories etc).
Doing it in a step by step ordered way should make things much easier
here.

NB: This should start with empty make.cfg, make.tmpl and maybe even
empty configure.in/host.cfg.in files in order to trim out all the
unnecessary bits.

9. Once it builds completely, we can then start on doing important
things like modularising the code properly. This also includes
putting BOOPSI back into Intuition.

10. Test modularity. Basically this would involve a build that
creates modules, whose only external references would be to C
library functions under Unix. Also no module should refer into
another modules directory at all. (Ie intuition and boopsi, 
the graphics subsystem - layers and graphics might be a problem 
here, but I hope not).

Urk
---
Anyway, that is the most detailed description I can really give without
going and doing something, which I can't really do until everybody is
happy with the idea.

[1] Of course you can use genmf to start with Makefile.src and create a
Makefile.inc

[2] This is a thing to talk about, should obviously machine dependant files
such as exec/cause.c exist in src/kernel/exec even in their brain-dead
stub form, or should they be somewhere else, so you could for example
copy this directory instead of trying to figure out what files to copy
from the exec directory? Stubs probably isn't a very good name either.

[3] This is probably how we should have done this in the first place,
and it would have made life much easier now too.

[4] Ie a Uni student.

Other peoples comments
----------------------
Note that these aren't really in any specific order, mostly because I
don't have the enthusiasm to go through it and do a really good job :-)

Aaron Digulla:

Well, I think something between MMake and pure make is neccessary. What
we need is basically an IMake-like system: The real makefiles are created
with the aid of some tool and then make takes over. This new tool must
allow this:

- Create syntactically correct makefiles

- Allow to modify global make variables (eg. CFLAGS) just for a single
makefile (that could basically mean "if the user sets CFLAGS in a
makefile, replace all occurencies of CFLAGS in this makefile with
xxx_CFLAGS"). But I don't think that this will be the biggest
problem: We need two things: Add flags to CFLAGS (can be done easily
with LOCAL_DEFINES and LOCAL_CFLAGS; they are assigned with := and
then no problems can happen.) and use own versions of CFLAGS. But we don't
need an arbitrary amount of CFLAGS. In the end, there are only three
types of CFLAGS: Compile code for the native OS, compile code which
mixes native and AROS calls and compile code inside/for AROS. So it
would be sufficient to have three CFLAGS variables, eg. NATIVE_CFLAGS,
CFLAGS and AROS_CFLAGS.

- It must be able to collect files from different directories (ie. a
VPATH-like functionality, maybe VPATH alone would be enough).

- The archtool could be omitted if we would put several files into small
packets (eg. a list-packet, a semaphore-packet, etc.) Then exec would
be 15 files instead of 150. That would be a good compromise between CVS
and GCC.

- It should be possible to rewrite MetaMake (MMake II ?) to create one single
Makefile instead of calling make itself. It would just mean to traverse the
build tree of MMake, put the include statements in the parent makefiles and
call make on time instead of for every makefile.

- Not quite. The basic idea for MetaMake was that you don't have to edit
a makefile if a new target/makefile is added. The new build process should
allow for the same. I really hate the way KDE does it: When you download
a second package, you have to compile it manually instead of beeing able to
say "make" at once place.

With MetaMake, you download the new package somewhere below AROS/ and
just say "mmake" and it gets compiled at the right place and time.

[Iain: Eh? I then commented that I could not see a use for this...]

Usually, someone adds a new directory and then he has to edit a makefile
in one of the parent dirs. With MetaMake, the tool itself searches for its
makefiles. This way, you can't forget something and you don't have to do
work which can be automated (basically, having to maintain parent makefiles
is clumsy and unneccessary).

[ Bernado Innocenti ]

Then  But how can you redefine TOP and CURDIR for each file? Consider that make
is going to process all them as a _single_ Makefile, and so the variables will
just the last value you give them...

What about using different variables for each subdir? I mean something like
EXEC_TOP, INTUITION_TOP, etc. Since make can't cd to the subproject directory,
we don't need a CURDIR variable anymore.

MetaMake will also create the real makefile from the template.

[Iain's reply:

It depends upon where you use them. If you redefine the values at the top
of the makefile and use them in the dependancies, then there is no problem,
since these are evaluated at the time the makefile is read. Using them in
commands is a no no. (Actually $TOP never changes).

CURDIR is also useful because it requires less typing, and makes moving 
directories easier (not that we really want to do that all that much).

]

I mean something easy like this:

    TOP_SUBDIRS = rom workbench demos games

    -include $(foreach i, $(TOP_SUBDIRS), $(i)/Makefile.inc)

Likewise, each makefile will include the makefiles in its subdirectories.
This way the user can speed up the build process by doing:

   make TOP_SUBDIRS=rom

Even better, all these #?_SUBDIRS stuff might be kept together in a single
place, let's say in AROS/config/config.mk. This way it would be easy to
switch the various targets on and off.

We could even make each Makefile.inc usable as a standalone Makefile.
I mean something like:

     cd rom/intuition ; make -f Makefile.inc

If we need common variables and rules, we could move them into the file
AROS/config/common.mk. Each sub-project will then do something like this:

    .ifndef COMMON_MK_INCLUDED
    -include ../../config/common.mk
    .endif

 (please forgive me, I can't remember the correct GNU make syntax for this).

[Iain:

That was something like what I intended, except that the rules like 
TOP_SUBDIRS would be defined in config/$(ARCH)-$(CPU)/Makefile.inc to enable
different architectures to build only the things that make sense to them.

The idea of each makefile.inc being a standalone file is quite an interesting
and useful idea though.

]


 What??? So you mean that when I'm working on some new Intuition feature
I should wait for half AROS to rebuild each time I want to recompile a file?

 I still think we should absolutely beek the dependencies to a risonable
minimum. Anyway, you may not hope to catch all the hidden dependencies, not
even using a single Makefile and generating dependencies with mkdepend.
Sometimes you change something in A that will break something else in B,
even if B is not #include'ing A.

[Iain:
Yes, but there should be nothing to recompile in all the other
directories, you would probably be talking about a very short wait
(a few seconds maybe - longer on Amiga's I guess)

].

 Well, in AROS there are already a lot of places where we need special
compiler flags for some files or some modules. Sometimes it's needed to
workaround bugs in the compiler, sometimes because you're going to compile
ROM code that needs to be relocatable or perhaps because there are unwanted
side effects in some optimizations.

[Iain:
Each top level directory should have its own set of flags. Ie the kernel,
the workbench/c, the workbench/libs etc.
]

To show an example of this, note that I had to turn off inlining in the init
code of native Amiga libraries because otherwise gcc would elect the
"return -1" library entry point as a possible candidate for inlining and
would move it last in the code segment. Now I've worked around this by
changing the entry point to an integer containg the machine code for
"moveq #-1,d0 ; rts", but it's still a horrible hack.

[Iain:
Isn't there a better way of doing this? I guess not otherwise you
would probably have used it :-)
]

 Unfortunately, when you write an OS kernel you sometimes _need_ to use
hacks. This is also true for NetBSD and Linux.

[Iain:
But I'd like to see as few of these as possible. FreeBSD has ONE set of 
compiler flags for everything and still manages to compile.
]

 Yes, it has been done like that in NetBSD. They also define DEBUG to
a number such as 1, 2 or 3, each identifying a different level of
verbosity. Then they use macros such as TRACE1, TRACE2 and TRACE3 that will
only produce debug output if DEBUG is defined to be greater or equal to that
number.

 We could have something like DEBUG_INTUITION, DEBUG_EXEC, and so on. If
we implement function level debugging, we should make sure that defining
DEBUG_EXEC also turns on debugging on ALL Exec's functions. We can obtain
this effect with a very complex trick:


  #ifdef DEBUG
      #define DEBUG_EXEC
  #endif

  #ifdef DEBUG_EXEC
      #ifndef DEBUG_EXEC_CacheClearE
          #define DEBUG_EXEC_CacheClearE
      #endif
      #ifndef DEBUG_EXEC_OpenLibrary
          #define DEBUG_EXEC_OpenLibrary
      #endif

      ...

  #endif

 Is there a better way? Or, is it possible to autogenerate this stuff
with some clever script?

[Iain: Lets hope so]

Booting
-------
A slightly different topic which has also come up during this discussion
is that of how to load AROS into memory. Currently there are two ways
in use. The first is a monolithic kernel like used by Unix systems, the
other method is to add entries into the system using the existing Exec,
but this will obviously only work on Amigas.

I like the idea of a bootloader as now exists in FreeBSD, which is loaded
into memory, parses some kind of configuration file, loads all sorts of things
into memory, then commits suicide after jumping to the kernel entry.

The thing about this is that it could be used on many different platforms
if written properly - simply separate the MD and MI parts. I mean we could
even reuse the InternalLoadSeg_XXX() code with a bit of trickery (well actually
by expanding the interface to include things as symbol table loading for 
debugging).

Anyway, here are a few comments from other people:

[Aaron Digulla]

In reply to Iain,

> What problems could we have. Easy the suggested structure here would mean
> that drivers are loaded into AROS after it has already started. This
> means basically we need a really clever bootloader for standalone systems,
> or some interesting glue code on emulated systems (which mind you is
> the way to go in my opinion).

What should be the problem ? AROS can load modules... if the harddisk
driver is already available :-)

I think QNX uses something like this: They have a primitive NVRAM "harddisk"
emulator (ie. they can create a filesystem in NVRAM and load things from it)
and it should be equally easy to write a harddisk emulator for a
filesystem in ROM or in the bootfile.

[Iain: The thing about bootloaders is that they often can use the primitives
available to other OS's. For example the PC BIOS gives us facilities for
reading the hard disks without even having to worry about whether they are
IDE, SCSI or whatever. Plus filesystem reading is MUCH simpler than filesystem
writing, so you don't need a full filesystem implementation.]

Well, I'd like to have a simple filesystem in the kernel because it's most
flexible *and* modular. To create a new kernel, you would create a file,
then a filesystem in that file, copy all parts of the OS into it and put the
bootloader in front of it. Then the bootloader would only have to contain
a very basic initialisation code plus the filesystem. The bootloader would
put itself in the list of available modules (the same code would probably be
used later to load the other modules when the whole system is up). So the
bootloader would be the "kernel" of the OS and the rest would be loaded on
demand.

[Bernardo:
 In other words, you are describing the current implementation of the Amiga
Kickstart. The bootstrap code is your bootloader, and the rest of the ROM
contains all the modules concatenated one after the other. Each one is
identified by a magic cookie (the resident tag). You can view this
as a very basic ROM filesystem. The boot code will just scan the ROM and
collect all the resident tags into a list sorted by priority. I guess this
could be done in a regular file too. If you also need to relocate the residents,
you could append reloc hunks to the resident tag just in a way similar to
executable files.
]

Filesystems which are loaded "later" can put themselves before the bootloader
which would allow to load newer versions of the drivers and to fall back
to the "compiled in" drivers if none can be found elsewhere.

[Bernardo:
 Yes, but how can you kick the old drivers out of your way? I think this
idea is rather complex to implement. Why would one need a "minimal" kickstart
that loads everything else from the hard disk and then commits suicide? ;-)
]

[Iain: Now that has some interesting ideas. In particular the idea of newer
disk based versions. I think this could easily be done simply by concatenating
all the modules into a file, called kernel, which is scanned by the loader
to create a list of modules, and later you can look elsewhere if a module
is yet to be loaded, and if you find a newer version then load that instead.
Although this way you cannot really reclaim the memory used by the kernel.]

[A (in reply):]

When a driver is not used anymore, you can unload it (that's what the HIDD
docs say and that's what should happen). If course, it won't be possible to
change the CPU "driver" while the system is up but that's not the point
(and virtually never neccessary). You need some drivers which must be in
ROM (eg. the drivers for the harddisk...) Others can be loaded from disk or
ROM. My point is that we could have a minimal driver in ROM and update them
on harddisk. By clever use of the search path, the new drivers on harddisk
would be used if the bootloader doesn't need the driver.

[I: It might be difficult to unload something that is loaded into the kernel
at boot time, since you would basically have to construct a list of what
pages of memory it uses.]

My point was this: I don't like drivers which depend on each other. I
especially don't like how Linux does it: When you want to add a new
driver to the kernel, you must recompile the whole thing. With my
approach, you could create an image which is basically a file with a
primitive filesystem inside. Adding a new driver would mean to compile
the driver and copy it into the primitive filesystem.

[Iain:
But if we really abstract the drivers so that they have to use other
HIDDs for interrupts and busses, then they will depend upon each other. I
would like to know the advantages of using a file with a primitive filesystem
over simply using a filesystem. I suppose the biggest one is security, in that
it would take more effort to overwrite a module in the PrimFS than in the 
real FS.
]

That's the theory. In practise, many drivers which you need and which
can be compiled as a module can't be loaded if the kernel has not been
compiled with the option to load that driver as a module (you will get
undefined symbols). That sucks. You should be able to compile a driver
completely independend if the rest of the kernel and load it *without*
having to recompile the whole kernel.

[I: Definitely, I think FreeBSD 3 can do this.]

[B: 
 That's because Linux (and NetBSD) modules are really object code that
is statically linked to the kernel at run time (funny ;-). On an
AmigaOS system, we don't need this. Our modules are in fact shared
libraries, handlers and devices. They can be loaded "on demand" and
even purged from memory when they are no longer in use. That's what
both Linux and NetBSD failed to provide with their monolithic kernels.

[AD:
Right. Even if the rest of the boot process is similar to Linux, this
must be Amiga-like :-)
]

 I still believe that the Amiga design beats UNIX on some important
aspects. The Amiga provides dynamic linking with very little overhead.
library jumptables, messages and IO requests all provide ways for
modules to interface with each other in a very indepentent way.

 I still can't see the difference between adding a file into a primitive
filesystem and appending a resident tag to a kickstart-like file.
Building a kickstart at West Chester was just a matter of concatenating
all parts together, being careful to support a backwards compatibility
hack called "kickety split".
]

[Bernardo is now chair :-)]
 Ok, don't call me mad. What about using a simple arhive format to store
the modules? Perhaps a very simple but common format, such as tar or zip.
vmlinuz is compressed by meands of gzip, and the small zlib implementation
in the bootload can decompress it. So we could have a "zkick" file, which
is just a zip archive (like Java classes) or perhaps a gzipped tar archive.
The module "bootstrap" is a plain executable that is joined with this
archive. Using a standard archive format allows us to rip out working
source code from any freeware program and put it inside "bootstrap".

 We could then extract and initialize all the modules just after
"bootstrap" has finished with the preliminary setup, and it would replace
the Kickstart code that scans the ROM for magic cookies in the original
AmigaOS.

 If instead we really need to be able to load the modules at run time, just
like their file based counterparts, we could even implement a "romlib" module
similar to the well known "ramlib". This one would patch itself into Exec's
OpenLibrary() and OpenDevice() to extract the modules from the archive
transparently.

[I: Or even make this standard behaviour for Exec. Actually I think this
part of the OS might need some overhaul. We would then have THREE different
ways of opening libraries. From memory, from disk, from bootimage. We are
now starting to get a bit messy with all the patching, there must be a nicer
way of doing it. Perhaps we could have a complete redesign of the image loading
system with dynamically loadable image loaders (for ELF, a.out, Amiga HUNK, etc)
and also for places to load modules from (memory, library/device path, bootfile
standard command path).
]
